<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5PZ3MK8QVV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5PZ3MK8QVV');
</script>
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Lookahead Anchoring</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="img/qual_1.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="912">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://lookahead-anchoring.github.io/">
    <meta property="og:title" content="Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation">
    <meta property="og:description" content="">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation">
    <!-- <meta name="twitter:description" content=""> -->
    <!-- <meta name="twitter:image" content="img/cats.png"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" type="image/x-icon" href="img/logo.ico">
    <!-- <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="./css/twentytwenty.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
    <script src="./js/jquery.twentytwenty.js"></script>




</head>


<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>Lookahead Anchoring</b>:<br> Preserving Character Identity in<br> Audio-Driven Human Animation<br>
                <small>
                    Arxiv 2025
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <a style="text-decoration:none" href="https://j0seo.github.io/">
                    Junyoung&nbsp;Seo<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://scholar.google.com/citations?user=46APmkYAAAAJ&hl=en">
                Zoe Landgraf<sup>2</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://miraodasilva.github.io/">
                    Rodrigo Mira<sup>2</sup>
                </a>
                <br>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://scholar.google.com/citations?user=qejRKDYAAAAJ&hl=en">
                    Alexandros Haliassos<sup>2</sup>
                </a>
                
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://stelabou.github.io/">
                    Stella Bounareli
                </a>
                
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://scholar.google.com/citations?user=HPwdvwEAAAAJ&hl=en">
                    Honglie Chen<sup>3</sup>
                </a>
                <br>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://www.linht.com/">
                    Linh Tran<sup>2</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://cvlab.kaist.ac.kr">
                    Seungryong&nbsp;Kim<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://scholar.google.com/citations?user=u8af3y8AAAAJ&hl=en">
                    Jie Shen<sup>2</sup>
                </a>
                <br>
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <sup>1</sup>KAIST
                        </td>
                        <td>
                            <sup>2</sup>Imperial College London
                        </td>
                        <td>
                            <sup>3</sup>University of Oxford
                        </td>
                    </tr>
                </table>
                <small>
                    <!-- <br> -->
                    <!-- *Work done during an internship.<br> -->
                    <!-- <sup>â€ </sup>Co-corresponding authors. -->
                </small>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/???" target="_blank">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <!-- <a href="" target="_blank"> -->
                            <a href="https://github.com/j0seo/lookahead-anchoring" target="_blank">
                                <image src="img/github.png" height="60px">
                                <h4><strong>Code & Weights</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

<!-- 
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <img src="./img/qual_1.png" width="100%">
                </div>

                <div class="text-justify">
                    Our model generates plausible novel views, conditioned on <b>only a single input view</b>, enabling to handle both in-domain images (top) and out-of-domain images (bottom).
                </div>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Application: Single Image to 3DGS</b>
                </h4>
                Our model can be applied to various downstream tasks. For example, given a single image, our model generates 3-4 novel view images, followed by feeding them into fast 3DGS reconstructors such as <a href="https://instantsplat.github.io">InstantSplat</a>. Then we can easily obtain a 3DGS scene <b>in 30 seconds</b>.
                <br>
                <div class="text-center">
                <video id="ide" width="70%" playsinline autoplay loop muted controls style="margin-top: 10px;">
                    <source src="video/slide.mp4" type="video/mp4" />
                </div>
            </video>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    <b>Warping-and-Inpainting vs. Ours</b>
                </h4>
                    <p class="text-justify">
                        We introduce a novel approach where a diffusion model <b>learns to implicitly conduct geometric warping</b> conditioned on MDE depth-based correspondence, instead of warping the pixels or the features directly. We design the model to interactively compensate for the ill-warped regions during its generation process, thereby <b>preventing artifacts typically caused by explicit warping</b>.
                    </p>
                    <div class="text-center">
                        <img src="./img/concept.jpg" width="85%">
                    </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    <b>GenWarp Knows Where to <i>Warp</i> and Where to <i>Refine</i>  </b>
                </h4>
                    <p class="text-justify">
                        In our augmented self-attention, the original self-attention part is more attentive to regions requiring generative priors, such as occluded or ill-warped areas (top), while the cross-view attention part focuses on regions that can be reliably warped from the input view (bottom). By aggregating both attentions at once, the model naturally <b>determines which regions to generate and which to warp.</b></p>
                    <div class="text-center">
                        <img src="./img/vis.png" width="85%">
                    </div>
            </div>
        </div>
        <br>


        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Qualitative Results on In-The-Wild Images</b>
                </h4>
                <div class="text-center">
                    <img src="./img/qual_ood.png" width="100%">
                </div>
            </div>
        </div>

        <br>



        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Overall Framework</b>
                </h4>
                <div class="text-justify">
                    Given an input view and a desired camera viewpoint, we obtain a pair of embeddings: a 2D coordinate embedding for the input view, 
                    and a <i>warped</i> coordinate embedding for the novel view.
                    With these embeddings, a semantic preserver network produces a semantic feature of the input view, 
                    and a diffusion model conditioned on them learns to conduct geometric warping 
                    to generate novel views.
                </div>
                <br>

                <div class="text-center">
                    <img src="./img/arch.png" width="85%">
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2"  style="margin-bottom: 5px;">
                <h4>
                    <b>Abstract</b>
                </h4>
                <div style="width: 90%;">
                    <!-- <img src="img/motivation_1.png" style="float: right; width: 40%; height: auto; margin-left: 20px;"> -->
                    <div class="text-justify">
                        Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose <b>Lookahead Anchoring</b>, which leverages keyframes from future timesteps <em>ahead</em> of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely.  We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures.
                    </div>
                </div>
                
                <br>
            </div>
        </div>



            <div class="row">
                <div class="col-md-8 col-md-offset-2" style="margin-bottom: 5px;">
                    <h4>
                       <b>Citation</b>
                    </h4>
                    <div class="form-group col-md-10 col-md-offset-1">
                        <textarea id="bibtex" class="form-control" readonly>
TBD
                        </textarea>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h4>
                        Acknowledgements
                    </h4>
                    <p class="text-justify">
                        TBD. The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                    </p>
                </div>
            </div>
        </div>


        </div>
        </div>


        


        


</body></html>
